# üïµÔ∏è An√°lise de Code Smells com LLMs (Ollama)

Este projeto cont√©m uma ferramenta de **Engenharia de Software** que utiliza Modelos de Linguagem (LLMs) rodando localmente via **Ollama** para auditar a qualidade do c√≥digo de um reposit√≥rio alvo.

O script:

1.  Mapeia os arquivos de c√≥digo-fonte (ex: `.ts`, `.tsx`, `.rs`, `.py`) do projeto configurado (ex: `screenpipe`).
2.  Submete trechos de c√≥digo a m√∫ltiplos modelos de IA (como `qwen2.5-coder`, `llama3.1`, `mistral`).
3.  Identifica e classifica **Code Smells** (como _Bloaters_, _Couplers_, _Dispensables_) baseando-se no cat√°logo do [Refactoring Guru](https://refactoring.guru/refactoring/smells).
4.  Gera relat√≥rios comparativos em `.csv`, contendo m√©tricas de tempo, contagem de problemas e sugest√µes de refatora√ß√£o.

---

## üöÄ Como Rodar o Projeto

Siga estas etapas para configurar o ambiente e executar a auditoria.

**Nota Importante:** A estrutura de pastas esperada √© que o script principal esteja dentro de `src/`. Todos os comandos abaixo devem ser executados **de dentro da pasta `src`**.

```text
Evolucao_Software_2025-2_screenpipe/
‚îú‚îÄ‚îÄ src/            <-- üìÇ Voc√™ deve estar aqui
‚îÇ   ‚îú‚îÄ‚îÄ resultados_corrigidos/
‚îÇ   ‚îú‚îÄ‚îÄ analise_smells.py
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt
‚îÇ   ‚îú‚îÄ‚îÄ .gitignore
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ ...
```

### 1. Pr√©-requisitos

Python 3.8+

Ollama instalado e em execu√ß√£o (Servidor de infer√™ncia local).

### 2. Configura√ß√£o do Ambiente

**1. Configura√ß√£o do Ollama:** Antes de iniciar o Python, voc√™ precisa garantir que os modelos de IA est√£o baixados na sua m√°quina. Abra seu terminal e execute:

```
ollama pull qwen2.5-coder:7b
ollama pull llama3.1:8b
ollama pull mistral

# Para confirmar a instala√ß√£o
ollama list
```

Certifique-se de que o servidor Ollama est√° rodando (padr√£o: localhost:11434).

**2. Crie e Ative um Ambiente Virtual:** √â altamente recomendado usar um ambiente virtual (venv) para isolar as depend√™ncias.

```
# Crie o ambiente (s√≥ precisa fazer isso uma vez)
python -m venv venv

# Ative o ambiente (precisa fazer toda vez que for rodar)
# No Windows:
.\venv\Scripts\activate
# No macOS / Linux:
source venv/bin/activate
```

**3. Instale as Depend√™ncias:** Com o ambiente ativado (voc√™ ver√° (venv) no seu terminal), instale as bibliotecas necess√°rias.

```
pip install -r requirements.txt
```

### 3. Configura√ß√£o do Script

O script j√° vem pr√©-configurado para analisar o reposit√≥rio screenpipe. Caso queira alterar o alvo ou os modelos, edite as vari√°veis no in√≠cio do arquivo analise_smells.py:

```
# ==============================================================================
# 1. CONFIGURA√á√ïES
# ==============================================================================
modelos_locais = [
    "qwen2.5-coder:7b",
    "llama3.1:8b",
    "mistral"
]

REPO_OWNER = "mediar-ai"
REPO_NAME  = "screenpipe"
```

### 4. Executando a An√°lise

Com o ambiente ativado `(venv)` e o Ollama rodando, execute:

```
python analise_smells.py
```

O script exibir√° o progresso no terminal:

- Ele iterar√° sobre a lista de modelos configurada.
- Voc√™ ver√° barras de progresso (tqdm) indicando a leitura e an√°lise de cada arquivo.
- Os resultados parciais s√£o salvos automaticamente para evitar perda de dados.

## üìä Sa√≠da (Resultados)

O script salvar√° os relat√≥rios automaticamente na pasta src/resultados_corrigidos/.
A estrutura final ficar√° assim:

```
src/
‚îú‚îÄ‚îÄ resultados_corrigidos/   <-- ‚úÖ SEUS RELAT√ìRIOS EST√ÉO AQUI
‚îÇ   ‚îú‚îÄ‚îÄ metricas_qwen2-5-coder_7b.csv
‚îÇ   ‚îú‚îÄ‚îÄ metricas_llama3-1_8b.csv
‚îÇ   ‚îú‚îÄ‚îÄ metricas_mistral.csv
‚îÇ
‚îî‚îÄ‚îÄ analise_smells.py
```

Cada arquivo .csv conter√° as seguintes colunas principais:

- `Modelo`: A IA utilizada (ex: mistral).
- `Arquivo`: O caminho do c√≥digo analisado.
- `Tempo_Seg`: Tempo de processamento (lat√™ncia) da an√°lise.
- `Total_Smells`: Quantidade total de problemas encontrados.
- `Categorias`: Colunas espec√≠ficas (Bloaters, Object-Orientation Abusers, etc.) com a contagem por tipo.
- `Analise_Raw`: A resposta completa da IA, contendo a explica√ß√£o t√©cnica e a sugest√£o de refatora√ß√£o ("Refactoring Recipe").

## üîß Customiza√ß√£o

_Para adicionar novos modelos:_ Baixe o modelo no Ollama (`ollama pull nome-modelo`) e adicione a string correspondente na lista `modelos_locais` dentro do script.

_Para alterar os tipos de arquivos:_ Edite a tupla `EXTENSOES_ALVO` (ex: adicione `.java` ou `.cpp`).
